#!/usr/bin/env python
# -*- coding: utf-8 -*-
import argparse
import os
import urllib
import urllib.request
import tempfile
import time 
import threading as thr
from os.path import join

import multiprocessing

from Constants import *

NUM_THREADS = 8
PRINT_EVERY = 25

def download_files(urls, output_path, thread_num):
    START_TRIM  = len ('https://')
    num_urls    = len (urls)
    
    for index, url in enumerate(urls):
        filename = url[START_TRIM:].replace('/', '.').strip()
        if index % PRINT_EVERY == 0:
            print (f"T{thread_num} ==> Downloading: {index}/{num_urls}...")
        download_file (url, os.path.join (output_path, filename))
            
    print (f"T{thread_num} ==> Done.")

def download_file(url, out_file):
    out_dir = os.path.dirname (out_file)
    if not os.path.isfile (out_file):
        fh, out_file_tmp = tempfile.mkstemp (dir=out_dir)
        f = os.fdopen (fh, 'w')
        f.close ()

        try:
            urllib.request.urlretrieve (url, out_file_tmp)
        except:
            print (f'Warning: error while downloading {out_file}, will try again...')
            time.sleep (0.2)
            download_file (url, out_file) # sad times

        os.rename (out_file_tmp, out_file)
    else:
        print (f'Warning: skipping download of existing file {out_file}.')

def multi_download_files(url_list, output_path):
    llen = len (url_list) // NUM_THREADS

    split_len   = len (url_list) // NUM_THREADS
    split_names = [ url_list[index * split_len : (index + 1) * split_len ] \
                    for index in range (NUM_THREADS + 1) ] 

    threads = []
    for index, urls in enumerate (split_names):
        thread = thr.Thread (
            target=download_files, 
            args=(urls,), 
            kwargs={ 'output_path' : output_path, 'thread_num' : index }, 
            daemon=True
        )

        threads.append (thread)
        thread.start ()

    for thread in threads:
        thread.join ()

def main():
    parser = argparse.ArgumentParser (description='Download Giphy GIFs from URLs generated by GenerateURLs.py.')
    parser.add_argument ('query', type=str)
    args = parser.parse_args ()

    url_query_path = os.path.join (URL_BASE, args.query)
    gif_query_path = os.path.join (GIF_BASE, args.query)

    if not os.path.exists (url_query_path):
        raise RuntimeError (f'No path {url_query_path}; try running GenerateURLS.py {args.query} <n>')
    if not os.path.exists (gif_query_path):
        os.makedirs (gif_query_path)

    for prefix in PREFIXES:
        url_path = os.path.join (url_query_path, f'{prefix}.txt')
        gif_path = os.path.join (gif_query_path, prefix)

        if not os.path.exists (url_path):
            raise RuntimeError (f'No path {url_path}; try deleting {url_query_path} and running GenerateURLs.py {args.query} <n>')
        if not os.path.exists (gif_path):
            os.makedirs (gif_path)

        with open (url_path, 'r') as f:
            multi_download_files(f.readlines (), gif_path)

if __name__ == "__main__":
    main()
